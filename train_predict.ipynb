{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree? robust with noise (especially if pruned), can handle irrelevant data\n",
    "# Naive bayes? not too good because of independence assumption\n",
    "# SVM? widely used, need to find the best kernel\n",
    "# nearest neighbors? data must be scaled, not too good with irrelevant features\n",
    "# neural net? requires a lot of time and a lot of data, can deal with irrelevant features, can overfit, local minima issues\n",
    "# ensemble?\n",
    "\n",
    "# decision tree, svm, nearest neigh, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# get clean data\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.preprocessing\n",
    "import sklearn.pipeline\n",
    "import sklearn.decomposition\n",
    "import sklearn.neighbors\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# need to pip install import_ipynb\n",
    "import import_ipynb\n",
    "from data_preperation import data\n",
    "\n",
    "# at this point, data should be clean \n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe do PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.loc[:, :'amfm_class']\n",
    "labels = data.loc[:, 'label']\n",
    "\n",
    "# 80%/20% split\n",
    "feat_train, feat_test, label_train, label_test = sk.model_selection.train_test_split(features, labels, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the best parameters for decision trees\n",
    "# TODO: don't need this function\n",
    "def find_best_params(feat_train, label_train):\n",
    "    best_acc = 0\n",
    "    best_crit = ''\n",
    "    best_depth = 1\n",
    "    best_imp_dec = 0\n",
    "    best_min_samples_leaf = 0\n",
    "    best_min_samples_split = 0\n",
    "    \"\"\"\n",
    "    for split_criterion in [\"best\", \"random\"]:\n",
    "        for max_depth in [1, 5, 10, 20]:\n",
    "            for min_impurity_decrease in [.0, .05, .1, .15]:\n",
    "                for min_samples_leaf in [1, 10, 50, 100]:\n",
    "                    for min_samples_split in [2, 4, 8, 10]:\n",
    "                        decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy', splitter=split_criterion, max_depth=max_depth, min_impurity_decrease=min_impurity_decrease, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split) # make model\n",
    "                        decision_tree.fit(feat_train, label_train) # train model\n",
    "\n",
    "                        label_predict = decision_tree.predict(feat_test) # predict labels of test data\n",
    "\n",
    "                        accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "                        if accuracy > best_acc:\n",
    "                            best_acc = accuracy\n",
    "                            best_crit = split_criterion\n",
    "                            best_depth = max_depth\n",
    "                            best_imp_dec = min_impurity_decrease\n",
    "                            best_min_samples_leaf =  min_samples_leaf\n",
    "                            best_min_samples_split = min_samples_split\n",
    "                        #print(\"Accuracy of the classifier on the test set with splitter={}, max_depth={}, min_impurity_decrease={}, min_samples_leaf={}, min_samples_split={}: {}\".format(split_criterion, max_depth, min_impurity_decrease, min_samples_leaf, min_samples_split, accuracy*100))\n",
    "    \n",
    "    \"\"\"\n",
    "    params = {\"max_depth\": [5,10,15,20],  \"min_samples_leaf\": [5,10,15,20], \"max_features\": [5,10,15], \"splitter\": [\"best\", \"random\"], \"min_impurity_decrease\":[.0, .05, .1, .15], \"min_samples_split\":[2, 4, 8, 10]}\n",
    "\n",
    "    grid_search = GridSearchCV(decision_tree, params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(features, labels)\n",
    "    # returns a map?\n",
    "    return grid_search.best_params_\n",
    "\n",
    "    #print(grid_search.best_params_)\n",
    "    #print(\"Accuracy of best params:\", grid_search.best_score_*100)\n",
    "    #return best_acc, best_crit, best_depth, best_imp_dec, best_min_samples_leaf, best_min_samples_split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy') # make model\n",
    "decision_tree.fit(feat_train, label_train) # train model\n",
    "\n",
    "label_predict = decision_tree.predict(feat_test) # predict labels of test data\n",
    "\n",
    "accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "print(\"Accuracy of simple decision tree: \", accuracy*100)\n",
    "\n",
    "# DOING CROSS VALIDATION \n",
    "\n",
    "# outer loop for CV\n",
    "decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy') # make model\n",
    "scores = sk.model_selection.cross_val_score(decision_tree, features, labels, cv=10) \n",
    "\n",
    "# find the best parameters for decision trees manually or using grid search, INNER CV LOOOP\n",
    "#best_params = find_best_params(feat_train, label_train)\n",
    "params = {\"max_depth\": [5,10,15,20],  \"min_samples_leaf\": [5,10,15,20], \"max_features\": [5,10,15], \"splitter\": [\"best\", \"random\"], \"min_impurity_decrease\":[.0, .05, .1, .15], \"min_samples_split\":[2, 4, 8, 10]}\n",
    "grid_search = GridSearchCV(decision_tree, params, cv=5, scoring='accuracy')\n",
    "grid_search.fit(features, labels)\n",
    "\n",
    "# make model with the best parameters, inner loop of CV\n",
    "\"\"\"\n",
    "decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy', \n",
    "                                               splitter=best_params['splitter'], \n",
    "                                               max_depth=best_params['max_depth'], \n",
    "                                               min_impurity_decrease=best_params['min_impurity_decrease'], \n",
    "                                               min_samples_leaf=best_params['min_samples_leaf'], \n",
    "                                               min_samples_split=best_params['min_samples_split'])\n",
    "\"\"\"\"\n",
    "#decision_tree.fit(feat_train, label_train)\n",
    "#label_predict = decision_tree.predict(feat_test)\n",
    "#accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "\n",
    "# inner & outer l\n",
    "decision_acc = sk.model_selection.cross_val_score(grid_search, features, labels, cv=10)\n",
    "\n",
    "print(\"Accuracy of decision tree with the best parameters and CV: \", decision_acc*100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "# TODO: maybe do confusion matrix??? Just to analyze model more, maybe roc curve is enough?\n",
    "# simple with CV:\n",
    "naive_bayes = sk.naive_bayes.GaussianNB()\n",
    "scores = sk.model_selection.cross_val_score(naive_bayes, features, labels, cv=10)\n",
    "\n",
    "print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "feat_train, feat_test, label_train, label_test = sk.model_selection.train_test_split(features, labels, test_size=0.2)\n",
    "naive_bayes = sk.naive_bayes.GaussianNB()\n",
    "naive_bayes.fit(feat_train, label_train)\n",
    "# This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. \n",
    "# Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the \n",
    "# probability that this datapoint is in class 1.\n",
    "proba = naive_bayes.predict_proba(feat_test)\n",
    "\n",
    "roc = sk.metrics.roc_curve(label_test, proba[:, 1])\n",
    "\n",
    "roc_auc = sk.metrics.roc_auc_score(label_test, proba[:, 1])\n",
    "acc = naive_bayes.score(feat_train, label_train)\n",
    "\n",
    "print(\"ROC AUC score, how good is this model?: \", roc_auc)\n",
    "print(\"Accuracy of this model: \", acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
