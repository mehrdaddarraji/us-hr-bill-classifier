{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree? robust with noise (especially if pruned), can handle irrelevant data\n",
    "# Naive bayes? not too good because of independence assumption\n",
    "# SVM? widely used, need to find the best kernel\n",
    "# nearest neighbors? data must be scaled, not too good with irrelevant features\n",
    "# neural net? requires a lot of time and a lot of data, can deal with irrelevant features, can overfit, local minima issues\n",
    "# ensemble?\n",
    "\n",
    "# decision tree, svm, nearest neigh, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data_preperation.ipynb\n",
      "   congress  bill sponsor_party sponsor_state  cosponsors  r_cosponsors  \\\n",
      "0       113     1             R            MI           0             0   \n",
      "1       113     2             R            NE          15            15   \n",
      "2       113     3             R            NE         134           132   \n",
      "3       113     4             R            MI           4             4   \n",
      "4       113     5             R            MN          12            12   \n",
      "\n",
      "   d_cosponsors                       subject  withdrawn_cosponsors  \\\n",
      "0             0                      Taxation                     0   \n",
      "1             0                        Energy                     0   \n",
      "2             2                        Energy                     1   \n",
      "3             0  Economics and Public Finance                     0   \n",
      "4             0                     Education                     0   \n",
      "\n",
      "   committees  subcommittees  actions  summary_words  \n",
      "0           1              0        1           2854  \n",
      "1           5              2       24           9938  \n",
      "2           3              4       59            502  \n",
      "3           9              1       26           7709  \n",
      "4           3              0       64           7303  \n",
      "0        0\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "5        1\n",
      "6        1\n",
      "9        1\n",
      "10       0\n",
      "11       0\n",
      "14       0\n",
      "19       0\n",
      "20       0\n",
      "21       0\n",
      "22       0\n",
      "23       1\n",
      "24       0\n",
      "25       0\n",
      "26       0\n",
      "27       0\n",
      "28       0\n",
      "29       0\n",
      "30       0\n",
      "31       0\n",
      "32       0\n",
      "33       0\n",
      "34       0\n",
      "35       0\n",
      "36       0\n",
      "37       0\n",
      "        ..\n",
      "19778    0\n",
      "19779    0\n",
      "19780    0\n",
      "19782    0\n",
      "19783    0\n",
      "19784    0\n",
      "19785    0\n",
      "19786    0\n",
      "19787    0\n",
      "19788    0\n",
      "19789    0\n",
      "19790    0\n",
      "19791    0\n",
      "19792    0\n",
      "19793    0\n",
      "19794    0\n",
      "19795    0\n",
      "19796    0\n",
      "19797    0\n",
      "19798    0\n",
      "19799    0\n",
      "19800    0\n",
      "19801    0\n",
      "19802    0\n",
      "19803    0\n",
      "19804    0\n",
      "19805    0\n",
      "19806    0\n",
      "19807    0\n",
      "19808    0\n",
      "Name: label, Length: 19707, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get clean data\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.pipeline\n",
    "import sklearn.decomposition\n",
    "import sklearn.neighbors\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# need to pip install import_ipynb\n",
    "import import_ipynb\n",
    "from data_preperation import features, labels\n",
    "\n",
    "# at this point, data should be clean \n",
    "print(features.head())\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>congress</th>\n",
       "      <th>bill</th>\n",
       "      <th>sponsor_party</th>\n",
       "      <th>sponsor_state</th>\n",
       "      <th>cosponsors</th>\n",
       "      <th>r_cosponsors</th>\n",
       "      <th>d_cosponsors</th>\n",
       "      <th>subject</th>\n",
       "      <th>withdrawn_cosponsors</th>\n",
       "      <th>committees</th>\n",
       "      <th>subcommittees</th>\n",
       "      <th>actions</th>\n",
       "      <th>summary_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>134</td>\n",
       "      <td>130</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   congress  bill  sponsor_party  sponsor_state  cosponsors  r_cosponsors  \\\n",
       "0         0     0              1             24           0             0   \n",
       "1         0     1              1             32          15            15   \n",
       "2         0     2              1             32         134           130   \n",
       "3         0     3              1             24           4             4   \n",
       "4         0     4              1             25          12            12   \n",
       "\n",
       "   d_cosponsors  subject  withdrawn_cosponsors  committees  subcommittees  \\\n",
       "0             0       30                     0           1              0   \n",
       "1             0       11                     0           5              2   \n",
       "2             2       11                     1           3              4   \n",
       "3             0        8                     0           9              1   \n",
       "4             0        9                     0           3              0   \n",
       "\n",
       "   actions  summary_words  \n",
       "0        0            933  \n",
       "1       23           1033  \n",
       "2       54            492  \n",
       "3       25           1020  \n",
       "4       56           1017  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe do PCA?\n",
    "# need to transform categorical data\n",
    "# 1. INSTANTIATE\n",
    "# encode labels with value between 0 and n_classes-1.\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "# 2/3. FIT AND TRANSFORM\n",
    "# use df.apply() to apply le.fit_transform to all columns\n",
    "features = features.apply(le.fit_transform)\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# 2. FIT\n",
    "enc.fit_transform(features)\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80%/20% split\n",
    "feat_train, feat_test, label_train, label_test = sk.model_selection.train_test_split(features, labels, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the best parameters for decision trees\n",
    "# TODO: don't need this function\n",
    "\n",
    "def find_best_params(feat_train, label_train):\n",
    "    best_acc = 0\n",
    "    best_crit = ''\n",
    "    best_depth = 1\n",
    "    best_imp_dec = 0\n",
    "    best_min_samples_leaf = 0\n",
    "    best_min_samples_split = 0\n",
    "    \"\"\"\n",
    "    for split_criterion in [\"best\", \"random\"]:\n",
    "        for max_depth in [1, 5, 10, 20]:\n",
    "            for min_impurity_decrease in [.0, .05, .1, .15]:\n",
    "                for min_samples_leaf in [1, 10, 50, 100]:\n",
    "                    for min_samples_split in [2, 4, 8, 10]:\n",
    "                        decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy', splitter=split_criterion, max_depth=max_depth, min_impurity_decrease=min_impurity_decrease, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split) # make model\n",
    "                        decision_tree.fit(feat_train, label_train) # train model\n",
    "\n",
    "                        label_predict = decision_tree.predict(feat_test) # predict labels of test data\n",
    "\n",
    "                        accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "                        if accuracy > best_acc:\n",
    "                            best_acc = accuracy\n",
    "                            best_crit = split_criterion\n",
    "                            best_depth = max_depth\n",
    "                            best_imp_dec = min_impurity_decrease\n",
    "                            best_min_samples_leaf =  min_samples_leaf\n",
    "                            best_min_samples_split = min_samples_split\n",
    "                        #print(\"Accuracy of the classifier on the test set with splitter={}, max_depth={}, min_impurity_decrease={}, min_samples_leaf={}, min_samples_split={}: {}\".format(split_criterion, max_depth, min_impurity_decrease, min_samples_leaf, min_samples_split, accuracy*100))\n",
    "    \n",
    "    \"\"\"\n",
    "    params = {\"max_depth\": [5,10,15,20],  \"min_samples_leaf\": [5,10,15,20], \"max_features\": [5,10,15], \"splitter\": [\"best\", \"random\"], \"min_impurity_decrease\":[.0, .05, .1, .15], \"min_samples_split\":[2, 4, 8, 10]}\n",
    "\n",
    "    grid_search = GridSearchCV(decision_tree, params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(features, labels)\n",
    "    # returns a map?\n",
    "    return grid_search.best_params_\n",
    "\n",
    "    #print(grid_search.best_params_)\n",
    "    #print(\"Accuracy of best params:\", grid_search.best_score_*100)\n",
    "    #return best_acc, best_crit, best_depth, best_imp_dec, best_min_samples_leaf, best_min_samples_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of simple decision tree:  96.24556062912227\n",
      "Accuracy of decision tree with the best parameters and CV:  96.49362456121374\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy') # make model\n",
    "decision_tree.fit(feat_train, label_train) # train model\n",
    "\n",
    "label_predict = decision_tree.predict(feat_test) # predict labels of test data\n",
    "\n",
    "accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "print(\"Accuracy of simple decision tree: \", accuracy*100)\n",
    "\n",
    "# DOING CROSS VALIDATION \n",
    "\n",
    "# outer loop for CV\n",
    "decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy') # make model\n",
    "\n",
    "scores = sk.model_selection.cross_val_score(decision_tree, features, labels, cv=10) \n",
    "\n",
    "# find the best parameters for decision trees manually or using grid search, INNER CV LOOOP\n",
    "#best_params = find_best_params(feat_train, label_train)\n",
    "params = {\"max_depth\": [5,10,15,20],  \"min_samples_leaf\": [5,10,15,20], \"max_features\": [5,10]}\n",
    "grid_search = GridSearchCV(decision_tree, params, cv=5, scoring='accuracy')\n",
    "grid_search.fit(features, labels)\n",
    "\n",
    "# make model with the best parameters, inner loop of CV\n",
    "\n",
    "#decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy', \n",
    "#                                               splitter=best_params['splitter'], \n",
    "#                                               max_depth=best_params['max_depth'], \n",
    "#                                               min_impurity_decrease=best_params['min_impurity_decrease'], \n",
    "#                                               min_samples_leaf=best_params['min_samples_leaf'], \n",
    "#                                               min_samples_split=best_params['min_samples_split'])\n",
    "\n",
    "#decision_tree.fit(feat_train, label_train)\n",
    "#label_predict = decision_tree.predict(feat_test)\n",
    "#accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "\n",
    "# inner & outer l\n",
    "decision_acc = sk.model_selection.cross_val_score(grid_search, features, labels, cv=10)\n",
    "\n",
    "print(\"Accuracy of decision tree with the best parameters and CV: \", decision_acc.mean()*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.86071127799798\n",
      "ROC AUC score, how good is this model?:  0.9495163940116158\n",
      "Accuracy of this model:  92.97811607992388\n"
     ]
    }
   ],
   "source": [
    "# naive bayes\n",
    "# TODO: maybe do confusion matrix??? Just to analyze model more, maybe roc curve is enough?\n",
    "# simple with CV:\n",
    "\n",
    "naive_bayes = sk.naive_bayes.GaussianNB()\n",
    "scores = sk.model_selection.cross_val_score(naive_bayes, features, labels, cv=10)\n",
    "\n",
    "print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "feat_train, feat_test, label_train, label_test = sk.model_selection.train_test_split(features, labels, test_size=0.2)\n",
    "naive_bayes = sk.naive_bayes.GaussianNB()\n",
    "naive_bayes.fit(feat_train, label_train)\n",
    "# This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. \n",
    "# Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the \n",
    "# probability that this datapoint is in class 1.\n",
    "proba = naive_bayes.predict_proba(feat_test)\n",
    "\n",
    "roc = sk.metrics.roc_curve(label_test, proba[:, 1])\n",
    "\n",
    "roc_auc = sk.metrics.roc_auc_score(label_test, proba[:, 1])\n",
    "acc = naive_bayes.score(feat_train, label_train)\n",
    "\n",
    "print(\"ROC AUC score, how good is this model?: \", roc_auc)\n",
    "print(\"Accuracy of this model: \", acc*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here1\n",
      "here2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "scaler_svm = StandardScaler()\n",
    "pca_redux_svm = PCA()\n",
    "svm_obj = SVC()\n",
    "pipe_svm = Pipeline([('scale', scaler_svm), ('pca', pca_redux_svm), ('svm', svm_obj)])\n",
    "print(\"start\")\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(5, 13)),\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "print(\"before grid\")\n",
    "grid_svm = GridSearchCV(pipe_svm, param_grid, cv=5)\n",
    "print(\"before predict\")\n",
    "pred_svm = cross_val_predict(grid_svm, features, labels, cv=5)\n",
    "print(\"done\")\n",
    "print(\"Accuracy:\", pred_svm.mean()*100)\n",
    "\n",
    "report_svm = classification_report(labels, pred_svm)\n",
    "print(report_svm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstandard_scaler = sk.preprocessing.StandardScaler()\\npca = sk.decomposition.PCA()\\nknn = sk.neighbors.KNeighborsClassifier(n_neighbors=7)\\npipeline = sk.pipeline.Pipeline(steps=[(\\'standard_scaler\\', standard_scaler), (\\'pca\\', pca), (\\'knn\\', knn)])\\n\\n# inner loop\\nscores = sk.model_selection.cross_val_score(pipeline, features, labels, cv=5)\\n#print(\"Accuracy:\", scores.mean()*100)\\n\\nparam_grid = {\\n    \\'pca__n_components\\': list(range(1, 14)),\\n    \\'knn__n_neighbors\\': list(range(1, 25))\\n}\\n\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5)\\ngrid_search.fit(features, labels)\\nprint(\"best params: \", grid_search.best_params_)\\n#print(\"Accuracy: \", grid_search.best_score_*100)\\n\\n# this does the nested loop\\nscores = sk.model_selection.cross_val_score(grid_search, features, labels, cv=5)\\n\\nprint(\"Accuracy:\", scores.mean()*100)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling\n",
    "\n",
    "standard_scaler = sk.preprocessing.StandardScaler()\n",
    "pca = sk.decomposition.PCA()\n",
    "knn = sk.neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "pipeline = sk.pipeline.Pipeline(steps=[('standard_scaler', standard_scaler), ('pca', pca), ('knn', knn)])\n",
    "\n",
    "# inner loop\n",
    "scores = sk.model_selection.cross_val_score(pipeline, features, labels, cv=5)\n",
    "#print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(1, 14)),\n",
    "    'knn__n_neighbors': list(range(1, 25))\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(features, labels)\n",
    "print(\"best params: \", grid_search.best_params_)\n",
    "#print(\"Accuracy: \", grid_search.best_score_*100)\n",
    "\n",
    "# this does the nested loop\n",
    "scores = sk.model_selection.cross_val_score(grid_search, features, labels, cv=5)\n",
    "\n",
    "print(\"Accuracy:\", scores.mean()*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler_nn = sk.preprocessing.StandardScaler()\n",
    "mlp_nn = MLPClassifier()\n",
    "pipe_nn = sk.pipeline.Pipeline([('scale', scaler_nn), ('nn', mlp_nn)])\n",
    "param_grid_nn = {\n",
    "    'nn__hidden_layer_sizes': [(30,),(40,),(50,),(60,)],\n",
    "    'nn__activation': ['logistic', 'tanh', 'relu']\n",
    "}\n",
    "grid_nn = GridSearchCV(pipe_nn, param_grid_nn, cv=5)\n",
    "pred_nn = cross_val_score(grid_nn, features, labels, cv=5)\n",
    "print(\"Accuracy:\", pred_nn.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params_rf = {\"max_depth\": list(range(35,56)), \"min_samples_leaf\": [8,10,12], \"max_features\": ['sqrt','log2']}\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestClassifier()\n",
    "grid_search_rf = GridSearchCV(rf, params_rf, cv=5)\n",
    "pred_rf = sk.model_selection.cross_val_score(grid_search_rf, features, labels, cv=5)\n",
    "\n",
    "print(\"Accuracy:\", pred_rf.mean()*100)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "boost_clf = AdaBoostClassifier(n_estimators = 150)\n",
    "\n",
    "pred_boost = cross_val_score(boost_clf, dataX, dataY, cv=5)\n",
    "\n",
    "print(\"Accuracy:\", pred_boost.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
