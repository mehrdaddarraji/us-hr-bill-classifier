{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree? robust with noise (especially if pruned), can handle irrelevant data\n",
    "# Naive bayes? not too good because of independence assumption\n",
    "# SVM? widely used, need to find the best kernel\n",
    "# nearest neighbors? data must be scaled, not too good with irrelevant features\n",
    "# neural net? requires a lot of time and a lot of data, can deal with irrelevant features, can overfit, local minima issues\n",
    "# ensemble?\n",
    "\n",
    "# decision tree, svm, nearest neigh, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   congress  bill sponsor_party sponsor_state  cosponsors  r_cosponsors  \\\n",
      "0       113     1             R            MI           0             0   \n",
      "1       113     2             R            NE          15            15   \n",
      "2       113     3             R            NE         134           132   \n",
      "3       113     4             R            MI           4             4   \n",
      "4       113     5             R            MN          12            12   \n",
      "\n",
      "   d_cosponsors                       subject  withdrawn_cosponsors  \\\n",
      "0             0                      Taxation                     0   \n",
      "1             0                        Energy                     0   \n",
      "2             2                        Energy                     1   \n",
      "3             0  Economics and Public Finance                     0   \n",
      "4             0                     Education                     0   \n",
      "\n",
      "   committees  subcommittees  actions  summary_words  \n",
      "0           1              0        1           2854  \n",
      "1           5              2       24           9938  \n",
      "2           3              4       59            502  \n",
      "3           9              1       26           7709  \n",
      "4           3              0       64           7303  \n"
     ]
    }
   ],
   "source": [
    "# get clean data\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.pipeline\n",
    "import sklearn.decomposition\n",
    "import sklearn.neighbors\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "# need to pip install import_ipynb\n",
    "import import_ipynb\n",
    "from data_preperation import features, labels\n",
    "\n",
    "\n",
    "# at this point, data should be clean \n",
    "print(features.head())\n",
    "#print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>congress</th>\n",
       "      <th>bill</th>\n",
       "      <th>sponsor_party</th>\n",
       "      <th>sponsor_state</th>\n",
       "      <th>cosponsors</th>\n",
       "      <th>r_cosponsors</th>\n",
       "      <th>d_cosponsors</th>\n",
       "      <th>subject</th>\n",
       "      <th>withdrawn_cosponsors</th>\n",
       "      <th>committees</th>\n",
       "      <th>subcommittees</th>\n",
       "      <th>actions</th>\n",
       "      <th>summary_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>134</td>\n",
       "      <td>130</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   congress  bill  sponsor_party  sponsor_state  cosponsors  r_cosponsors  \\\n",
       "0         0     0              1             24           0             0   \n",
       "1         0     1              1             32          15            15   \n",
       "2         0     2              1             32         134           130   \n",
       "3         0     3              1             24           4             4   \n",
       "4         0     4              1             25          12            12   \n",
       "\n",
       "   d_cosponsors  subject  withdrawn_cosponsors  committees  subcommittees  \\\n",
       "0             0       30                     0           1              0   \n",
       "1             0       11                     0           5              2   \n",
       "2             2       11                     1           3              4   \n",
       "3             0        8                     0           9              1   \n",
       "4             0        9                     0           3              0   \n",
       "\n",
       "   actions  summary_words  \n",
       "0        0            933  \n",
       "1       23           1033  \n",
       "2       54            492  \n",
       "3       25           1020  \n",
       "4       56           1017  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe do PCA?\n",
    "# need to transform categorical data\n",
    "# 1. INSTANTIATE\n",
    "# encode labels with value between 0 and n_classes-1.\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "# 2/3. FIT AND TRANSFORM\n",
    "# use df.apply() to apply le.fit_transform to all columns\n",
    "features = features.apply(le.fit_transform)\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# 2. FIT\n",
    "enc.fit_transform(features)\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3816\n",
      "1     184\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 80%/20% split\n",
    "features, labels = resample(features, labels, n_samples=5000)\n",
    "feat_train, feat_test, label_train, label_test = sk.model_selection.train_test_split(features, labels, test_size=0.2)\n",
    "#print(feat_train.columns)\n",
    "#print(label_train.value_counts())\n",
    "#print(type(label_train))\n",
    "#print(type(feat_train))\n",
    "#sm = SMOTE()\n",
    "#feat_train, label_train = sm.fit_sample(feat_train, label_train)\n",
    "#feat_train, label_train = resample(feat_train, label_train, n_samples=5000)\n",
    "#sm = SMOTENC(random_state=42, categorical_features=[18, 19])\n",
    "#feat_train, label_train = SMOTE().fit_resample(feat_train, label_train)\n",
    "#x = np.linspace(label_train)\n",
    "#print(label_train)\n",
    "\n",
    "#y = pd.DataFrame.from_records(label_train)\n",
    "#print(pd.value_counts(y.values.flatten()))\n",
    "#print(y.value_counts())\n",
    "#print(len(feat_train))\n",
    "#pd.value_counts(df.values.flatten())\n",
    "\n",
    "print(label_train.value_counts())\n",
    "#smt = SMOTE(random_state=0)\n",
    "#feat_resampled, label_resampled = smote.fit_sample(feat_train, label_train)\n",
    "\n",
    "#feat_train = pd.DataFrame(feat_resampled, columns=feat_train.columns)\n",
    "#label_train = pd.Series(label_resampled)\n",
    "#print(label_resampled.head())\n",
    "\n",
    "#print(label_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def decision_tree_no_SMOTE():\n",
    "    print(\"Decision Tree, no SMOTE\")\n",
    "    decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy') # make model\n",
    "    decision_tree.fit(feat_train, label_train) # train model\n",
    "\n",
    "    label_predict = decision_tree.predict(feat_test) # predict labels of test data\n",
    "\n",
    "    accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "    print(\"Accuracy of simple decision tree: \", accuracy*100)\n",
    "\n",
    "    # DOING CROSS VALIDATION \n",
    "    #smt = SMOTE()\n",
    "    # outer loop for CV\n",
    "    decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy') # make model\n",
    "\n",
    "    scores = sk.model_selection.cross_val_score(decision_tree, features, labels, cv=10) \n",
    "\n",
    "    # find the best parameters for decision trees manually or using grid search, INNER CV LOOOP\n",
    "    #best_params = find_best_params(feat_train, label_train)\n",
    "    params = {\"decision_tree__max_depth\": [5,10,15,20],  \n",
    "              \"decision_tree__min_samples_leaf\": [5,10,15,20], \n",
    "              \"decision_tree__max_features\": [5,10]}\n",
    "              #'smt__k_neighbors': list(range(1, 16, 2))\n",
    "\n",
    "\n",
    "    #smt = SMOTE(ratio=.5)\n",
    "    #pipeline = sk.pipeline.Pipeline(steps=[('smt', smt), ('decision_tree', decision_tree)])\n",
    "    #pipeline = Pipeline([('smt', smt), ('decision_tree', decision_tree)])\n",
    "    pipeline = Pipeline([('decision_tree', decision_tree)])\n",
    "    grid_search = GridSearchCV(pipeline, params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(features, labels)\n",
    "    print(\"best params: \", grid_search.best_params_)\n",
    "\n",
    "    # make model with the best parameters, inner loop of CV\n",
    "\n",
    "    #decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy', \n",
    "    #                                               splitter=best_params['splitter'], \n",
    "    #                                               max_depth=best_params['max_depth'], \n",
    "    #                                               min_impurity_decrease=best_params['min_impurity_decrease'], \n",
    "    #                                               min_samples_leaf=best_params['min_samples_leaf'], \n",
    "    #                                               min_samples_split=best_params['min_samples_split'])\n",
    "\n",
    "    #decision_tree.fit(feat_train, label_train)\n",
    "    #label_predict = decision_tree.predict(feat_test)\n",
    "    #accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "\n",
    "    # inner & outer l\n",
    "    decision_acc = sk.model_selection.cross_val_score(grid_search, features, labels, cv=5)\n",
    "\n",
    "    print(\"Accuracy of decision tree with the best parameters and CV: \", decision_acc.mean()*100)\n",
    "\n",
    "    labels_predict = sk.model_selection.cross_val_predict(grid_search, features, labels, cv=5)\n",
    "    conf_mat = sklearn.metrics.confusion_matrix(labels, labels_predict)\n",
    "    print(\"confusion matrix: \\n\", conf_mat)\n",
    "\n",
    "    report = sklearn.metrics.classification_report(labels, labels_predict)\n",
    "    print(\"\\nclassification report:\\n\", report)\n",
    "\n",
    "    decision_tree.fit(feat_train, label_train)\n",
    "    # This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. \n",
    "    # Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the \n",
    "    # probability that this datapoint is in class 1.\n",
    "    proba = decision_tree.predict_proba(feat_test)\n",
    "\n",
    "    #print(label_test)\n",
    "    #print(proba[:, 1])\n",
    "\n",
    "    roc = sk.metrics.roc_curve(label_test, proba[:, 1])\n",
    "    roc_auc = sk.metrics.roc_auc_score(label_test, proba[:, 1])\n",
    "\n",
    "    print(\"ROC AUC score, how good is this model?: \", roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def decision_tree_SMOTE():\n",
    "    print(\"Decision Tree, SMOTE\")\n",
    "    decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy') # make model\n",
    "    decision_tree.fit(feat_train, label_train) # train model\n",
    "\n",
    "    label_predict = decision_tree.predict(feat_test) # predict labels of test data\n",
    "\n",
    "    accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "    print(\"Accuracy of simple decision tree: \", accuracy*100)\n",
    "\n",
    "    # DOING CROSS VALIDATION \n",
    "    smt = SMOTE()\n",
    "    # outer loop for CV\n",
    "    decision_tree = sk.tree.DecisionTreeClassifier(criterion='entropy') # make model\n",
    "\n",
    "    scores = sk.model_selection.cross_val_score(decision_tree, features, labels, cv=10) \n",
    "\n",
    "    # find the best parameters for decision trees manually or using grid search, INNER CV LOOOP\n",
    "    #best_params = find_best_params(feat_train, label_train)\n",
    "    params = {\"decision_tree__max_depth\": [5,10,15,20],  \n",
    "              \"decision_tree__min_samples_leaf\": [5,10,15,20], \n",
    "              \"decision_tree__max_features\": [5,10],\n",
    "              'smt__k_neighbors': list(range(1, 16, 2))}\n",
    "\n",
    "\n",
    "    #smt = SMOTE(ratio=.5)\n",
    "    #pipeline = sk.pipeline.Pipeline(steps=[('smt', smt), ('decision_tree', decision_tree)])\n",
    "    pipeline = Pipeline([('smt', smt), ('decision_tree', decision_tree)])\n",
    "    grid_search = GridSearchCV(pipeline, params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(features, labels)\n",
    "    print(\"best params: \", grid_search.best_params_)\n",
    "\n",
    "    # inner & outer l\n",
    "    decision_acc = sk.model_selection.cross_val_score(grid_search, features, labels, cv=5)\n",
    "\n",
    "    print(\"Accuracy of decision tree with the best parameters and CV: \", decision_acc.mean()*100)\n",
    "\n",
    "    labels_predict = sk.model_selection.cross_val_predict(grid_search, features, labels, cv=5)\n",
    "    conf_mat = sklearn.metrics.confusion_matrix(labels, labels_predict)\n",
    "    print(\"confusion matrix: \\n\", conf_mat)\n",
    "\n",
    "    report = sklearn.metrics.classification_report(labels, labels_predict)\n",
    "    print(\"\\nclassification report:\\n\", report)\n",
    "\n",
    "    decision_tree.fit(feat_train, label_train)\n",
    "    proba = decision_tree.predict_proba(feat_test)\n",
    "\n",
    "    roc = sk.metrics.roc_curve(label_test, proba[:, 1])\n",
    "    roc_auc = sk.metrics.roc_auc_score(label_test, proba[:, 1])\n",
    "\n",
    "    print(\"ROC AUC score, how good is this model?: \", roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "# TODO: maybe do confusion matrix??? Just to analyze model more, maybe roc curve is enough?\n",
    "# simple with CV:\n",
    "def naive_bayes_no_SMOTE():\n",
    "    print(\"Naive Bayes, no SMOTE\")\n",
    "    naive_bayes = sk.naive_bayes.GaussianNB()\n",
    "    scores = sk.model_selection.cross_val_score(naive_bayes, features, labels, cv=10)\n",
    "\n",
    "    print(\"simple model Accuracy:\", scores.mean()*100)\n",
    "\n",
    "    #feat_train, feat_test, label_train, label_test = sk.model_selection.train_test_split(features, labels, test_size=0.2)\n",
    "    naive_bayes = sk.naive_bayes.GaussianNB()\n",
    "\n",
    "\n",
    "    scores = sk.model_selection.cross_val_score(naive_bayes, features, labels, cv=10)\n",
    "    #params = {\n",
    "    #          'smt__k_neighbors': list(range(1, 16, 2))}\n",
    "\n",
    "\n",
    "    #smt = SMOTE()\n",
    "    #pipeline = Pipeline([('smt', smt), ('naive_bayes', naive_bayes)])\n",
    "    pipeline = Pipeline([('naive_bayes', naive_bayes)])\n",
    "    #grid_search = GridSearchCV(pipeline, params, cv=5, scoring='accuracy')\n",
    "    #grid_search = GridSearchCV(pipeline, cv=5, scoring='accuracy')\n",
    "    #grid_search.fit(features, labels)\n",
    "    #print(\"best params: \", grid_search.best_params_)\n",
    "\n",
    "    naive_bayes.fit(feat_train, label_train)\n",
    "    decision_acc = sk.model_selection.cross_val_score(naive_bayes, features, labels, cv=5)\n",
    "\n",
    "    print(\"Accuracy of decision tree with the best parameters and CV: \", decision_acc.mean()*100)\n",
    "\n",
    "    labels_predict = sk.model_selection.cross_val_predict(naive_bayes, features, labels, cv=10)\n",
    "    conf_mat = sklearn.metrics.confusion_matrix(labels, labels_predict)\n",
    "    print(\"confusion matrix: \\n\", conf_mat)\n",
    "\n",
    "    report = sklearn.metrics.classification_report(labels, labels_predict)\n",
    "    print(\"\\nclassification report:\\n\", report)\n",
    "\n",
    "    naive_bayes.fit(feat_train, label_train)\n",
    "    # This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. \n",
    "    # Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the \n",
    "    # probability that this datapoint is in class 1.\n",
    "    proba = naive_bayes.predict_proba(feat_test)\n",
    "\n",
    "    #print(label_test)\n",
    "    #print(proba[:, 1])\n",
    "\n",
    "    roc = sk.metrics.roc_curve(label_test, proba[:, 1])\n",
    "    roc_auc = sk.metrics.roc_auc_score(label_test, proba[:, 1])\n",
    "\n",
    "    print(\"ROC AUC score, how good is this model?: \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "# TODO: maybe do confusion matrix??? Just to analyze model more, maybe roc curve is enough?\n",
    "# simple with CV:\n",
    "def naive_bayes_SMOTE():\n",
    "    print(\"Naive Bayes, SMOTE\")\n",
    "    naive_bayes = sk.naive_bayes.GaussianNB()\n",
    "    scores = sk.model_selection.cross_val_score(naive_bayes, features, labels, cv=10)\n",
    "\n",
    "    print(\"simple model Accuracy:\", scores.mean()*100)\n",
    "\n",
    "    #feat_train, feat_test, label_train, label_test = sk.model_selection.train_test_split(features, labels, test_size=0.2)\n",
    "    naive_bayes = sk.naive_bayes.GaussianNB()\n",
    "\n",
    "\n",
    "    scores = sk.model_selection.cross_val_score(naive_bayes, features, labels, cv=10)\n",
    "    params = {\n",
    "              'smt__k_neighbors': list(range(1, 16, 2))}\n",
    "\n",
    "\n",
    "    smt = SMOTE()\n",
    "    pipeline = Pipeline([('smt', smt), ('naive_bayes', naive_bayes)])\n",
    "    grid_search = GridSearchCV(pipeline, params, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(features, labels)\n",
    "    print(\"best params: \", grid_search.best_params_)\n",
    "\n",
    "    naive_bayes.fit(feat_train, label_train)\n",
    "    decision_acc = sk.model_selection.cross_val_score(naive_bayes, features, labels, cv=5)\n",
    "\n",
    "    print(\"Accuracy of decision tree with the best parameters and CV: \", decision_acc.mean()*100)\n",
    "\n",
    "    labels_predict = sk.model_selection.cross_val_predict(naive_bayes, features, labels, cv=10)\n",
    "    conf_mat = sklearn.metrics.confusion_matrix(labels, labels_predict)\n",
    "    print(\"confusion matrix: \\n\", conf_mat)\n",
    "\n",
    "    report = sklearn.metrics.classification_report(labels, labels_predict)\n",
    "    print(\"\\nclassification report:\\n\", report)\n",
    "\n",
    "    naive_bayes.fit(feat_train, label_train)\n",
    "    # This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. \n",
    "    # Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the \n",
    "    # probability that this datapoint is in class 1.\n",
    "    proba = naive_bayes.predict_proba(feat_test)\n",
    "\n",
    "    roc = sk.metrics.roc_curve(label_test, proba[:, 1])\n",
    "    roc_auc = sk.metrics.roc_auc_score(label_test, proba[:, 1])\n",
    "\n",
    "    print(\"ROC AUC score, how good is this model?: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "def nearest_neighbors_no_SMOTE():\n",
    "    print(\"scaling...\")\n",
    "    standard_scaler = sk.preprocessing.StandardScaler()\n",
    "    pca = sk.decomposition.PCA()\n",
    "    knn = sk.neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "    knn.fit(feat_train, label_train)\n",
    "    label_predict = knn.predict(feat_test) # predict labels of test data\n",
    "\n",
    "    accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "    print(\"Accuracy of simple KNN: \", accuracy*100)\n",
    "\n",
    "    knn = sk.neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "    #smt = SMOTE()\n",
    "    #pipeline = Pipeline([('smt', smt), ('standard_scaler', standard_scaler), ('pca', pca), ('knn', knn)])\n",
    "    pipeline = Pipeline([('standard_scaler', standard_scaler), ('pca', pca), ('knn', knn)])\n",
    "    #pipeline = sk.pipeline.Pipeline(steps=[('standard_scaler', standard_scaler), ('pca', pca), ('knn', knn)])\n",
    "    print(\"finished scaling.\")\n",
    "    # inner loop\n",
    "    print(\"starting inner loop...\")\n",
    "    scores = sk.model_selection.cross_val_score(pipeline, features, labels, cv=5)\n",
    "    #print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "    param_grid = {\n",
    "        'pca__n_components': list(range(1, 14)),\n",
    "        'knn__n_neighbors': list(range(1, 26, 2))}\n",
    "        #'smt__k_neighbors': list(range(1, 26, 2))\n",
    "\n",
    "\n",
    "    # param_grid=  {'knn__n_neighbors': [1], 'pca__n_components': [13], 'smt__k_neighbors': [1]}\n",
    "\n",
    "    #param_grid = {\n",
    "    #    'pca__n_components': [12],\n",
    "    #    'knn__n_neighbors': [15]\n",
    "    #}\n",
    "\n",
    "    print(\"starting grid search...\")\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    grid_search.fit(features, labels)\n",
    "    print(\"best params: \", grid_search.best_params_)\n",
    "    #print(\"Accuracy: \", grid_search.best_score_*100)\n",
    "\n",
    "    # this does the nested loop\n",
    "    scores = sk.model_selection.cross_val_score(grid_search, features, labels, cv=5, n_jobs=-1)\n",
    "\n",
    "    print(\"Accuracy with best params:\", scores.mean()*100)\n",
    "\n",
    "    # CONFUSION MATRIX EVALUATION\n",
    "    labels_predict = sk.model_selection.cross_val_predict(grid_search, features, labels, cv=5, n_jobs=-1)\n",
    "    conf_mat = sklearn.metrics.confusion_matrix(labels, labels_predict)\n",
    "    print(\"confusion matrix: \\n\", conf_mat)\n",
    "\n",
    "    report = sklearn.metrics.classification_report(labels, labels_predict)\n",
    "    print(\"\\nclassification report:\\n\", report)\n",
    "\n",
    "    knn.fit(feat_train, label_train)\n",
    "    proba = knn.predict_proba(feat_test)\n",
    "\n",
    "    # ROC CURVE EVALUATION\n",
    "    roc = sk.metrics.roc_curve(label_test, proba[:, 1])\n",
    "    roc_auc = sk.metrics.roc_auc_score(label_test, proba[:, 1])\n",
    "\n",
    "    print(\"ROC AUC score, how good is this model?: \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "def nearest_neighbors_SMOTE():\n",
    "    print(\"Nearest Neighbors, SMOTE\")\n",
    "    print(\"scaling...\")\n",
    "    standard_scaler = sk.preprocessing.StandardScaler()\n",
    "    pca = sk.decomposition.PCA()\n",
    "    knn = sk.neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "    knn.fit(feat_train, label_train)\n",
    "    label_predict = knn.predict(feat_test) # predict labels of test data\n",
    "\n",
    "    accuracy = sk.metrics.accuracy_score(label_test, label_predict)\n",
    "    print(\"Accuracy of simple KNN: \", accuracy*100)\n",
    "\n",
    "    knn = sk.neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "    smt = SMOTE()\n",
    "    pipeline = Pipeline([('smt', smt), ('standard_scaler', standard_scaler), ('pca', pca), ('knn', knn)])\n",
    "    print(\"finished scaling.\")\n",
    "    # inner loop\n",
    "    print(\"starting inner loop...\")\n",
    "    scores = sk.model_selection.cross_val_score(pipeline, features, labels, cv=5)\n",
    "    #print(\"Accuracy:\", scores.mean()*100)\n",
    "\n",
    "    param_grid = {\n",
    "        'pca__n_components': list(range(1, 14)),\n",
    "        'knn__n_neighbors': list(range(1, 26, 2)),\n",
    "        'smt__k_neighbors': list(range(1, 26, 2))}\n",
    "\n",
    "    print(\"starting grid search...\")\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    grid_search.fit(features, labels)\n",
    "    print(\"best params: \", grid_search.best_params_)\n",
    "    #print(\"Accuracy: \", grid_search.best_score_*100)\n",
    "\n",
    "    # this does the nested loop\n",
    "    scores = sk.model_selection.cross_val_score(grid_search, features, labels, cv=5, n_jobs=-1)\n",
    "\n",
    "    print(\"Accuracy with best params:\", scores.mean()*100)\n",
    "\n",
    "    # CONFUSION MATRIX EVALUATION\n",
    "    labels_predict = sk.model_selection.cross_val_predict(grid_search, features, labels, cv=5, n_jobs=-1)\n",
    "    conf_mat = sklearn.metrics.confusion_matrix(labels, labels_predict)\n",
    "    print(\"confusion matrix: \\n\", conf_mat)\n",
    "\n",
    "    report = sklearn.metrics.classification_report(labels, labels_predict)\n",
    "    print(\"\\nclassification report:\\n\", report)\n",
    "\n",
    "    knn.fit(feat_train, label_train)\n",
    "    proba = knn.predict_proba(feat_test)\n",
    "\n",
    "    # ROC CURVE EVALUATION\n",
    "    roc = sk.metrics.roc_curve(label_test, proba[:, 1])\n",
    "    roc_auc = sk.metrics.roc_auc_score(label_test, proba[:, 1])\n",
    "\n",
    "    print(\"ROC AUC score, how good is this model?: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree, no SMOTE\n",
      "Accuracy of simple decision tree:  96.3\n",
      "best params:  {'decision_tree__max_depth': 5, 'decision_tree__max_features': 10, 'decision_tree__min_samples_leaf': 15}\n",
      "Accuracy of decision tree with the best parameters and CV:  96.85963739963739\n",
      "confusion matrix: \n",
      " [[4713   70]\n",
      " [ 115  102]]\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      4783\n",
      "           1       0.59      0.47      0.52       217\n",
      "\n",
      "    accuracy                           0.96      5000\n",
      "   macro avg       0.78      0.73      0.75      5000\n",
      "weighted avg       0.96      0.96      0.96      5000\n",
      "\n",
      "ROC AUC score, how good is this model?:  0.7179655918021999\n",
      "Naive Bayes, no SMOTE\n",
      "simple model Accuracy: 93.47978407913632\n",
      "Accuracy of decision tree with the best parameters and CV:  93.39973485973486\n",
      "confusion matrix: \n",
      " [[4522  261]\n",
      " [  65  152]]\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      4783\n",
      "           1       0.37      0.70      0.48       217\n",
      "\n",
      "    accuracy                           0.93      5000\n",
      "   macro avg       0.68      0.82      0.72      5000\n",
      "weighted avg       0.96      0.93      0.94      5000\n",
      "\n",
      "ROC AUC score, how good is this model?:  0.9638055842812823\n",
      "scaling...\n",
      "Accuracy of simple KNN:  96.5\n",
      "finished scaling.\n",
      "starting inner loop...\n",
      "starting grid search...\n",
      "best params:  {'knn__n_neighbors': 11, 'pca__n_components': 11}\n"
     ]
    }
   ],
   "source": [
    "decision_tree_no_SMOTE()\n",
    "naive_bayes_no_SMOTE()\n",
    "nearest_neighbors_no_SMOTE()\n",
    "\n",
    "decision_tree_SMOTE()\n",
    "naive_bayes_SMOTE()\n",
    "nearest_neighbors_SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
